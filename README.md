ğŸ“˜ Transformer Attention Visualization using Hugging Face BERT

Understanding Self-Attention Through Interactive & Static Visualizations

This project demonstrates how to extract, analyze, and visualize self-attention from a pre-trained BERT model using Hugging Face Transformers, PyTorch, BertViz, and Matplotlib.
You learn how BERT â€œlooksâ€ at different tokens, how attention changes across layers and heads, and how to build meaningful visualizations for NLP interpretability.

ğŸš€ Overview

Transformers power most modern NLP systems, and self-attention is their core mechanism.
In this project, you will:

Load a BERT base model (bert-base-uncased)

Extract its attention tensors

Visualize attention using:
âœ” BertViz (interactive)
âœ” Matplotlib heatmaps
âœ” Token-focused attention plots

Interpret why certain words attend to others

Understand how BERT processes context and meaning

This project is based on my notebook and documentation:
â€œVisualize Transformer Attention using Hugging Face BERTâ€.

ğŸ“‚ Tech Stack
Layer	Tools Used	Purpose
Model Hub	Hugging Face Transformers	Load BERT, return attentions
Backend	PyTorch	Model inference
Visualization (Interactive)	BertViz	Explore layers Ã— heads
Visualization (Static)	Matplotlib, NumPy	Heatmaps & token-level charts
Runtime	Google Colab / Local Python	GPU support & easy execution
ğŸ“¦ Installation
Install Requirements
pip install -U transformers torch bertviz matplotlib numpy

ğŸ§  Key Concepts You Learn
âœ” BERTâ€™s attention structure

Each attention tensor has shape:
(layers, heads, seq_len, seq_len)

âœ” Understanding attention

Row i â†’ where token i sends attention

Column j â†’ how much token j is attended to

Deeper layers = more semantic patterns

Heads specialize (syntax, position, disambiguation)

âœ” Visualizing attention

Interactive graph for each head

Heatmap per layer/head

Token-specific attention distribution (e.g., â€œbankâ€ â†’ â€œdepositâ€, â€œmoneyâ€)

ğŸ““ Notebook Walkthrough
1ï¸âƒ£ Install & import dependencies
2ï¸âƒ£ Load tokenizer & model with output_attentions=True
3ï¸âƒ£ Tokenize sample sentence

Example:

"He went to the bank to deposit money."

4ï¸âƒ£ Run BERT forward pass

Extract:

attentions[layer][batch, head, from, to]

5ï¸âƒ£ Interactive Visualization (BertViz)

Explore 12 layers Ã— 12 heads
Click any head to view token-to-token attention links.

6ï¸âƒ£ Static Heatmaps

Plot attention matrices using Matplotlib.

7ï¸âƒ£ Token-specific attention

See which tokens a specific word (e.g., bank) attends to strongly.

ğŸ“Š Example Outcomes
âœ” Find which tokens clarify meaning

For ambiguous words like bank, attention highlights contextual words:
deposit, money

âœ” Visualize global context

Special tokens like [CLS] gather global sentence information.

âœ” Understand specialization

Some heads track structure, others track long-range dependencies.

â–¶ï¸ Running Instructions
Google Colab (Recommended)

Upload .ipynb

Run cells sequentially

Explore attention using BertViz

Local Python
python Transformer_Attention.ipynb


(open in Jupyter or VS Code)

ğŸ“ Suggested Repository Structure
ğŸ“¦ transformer-attention-bert
 â”£ ğŸ“„ README.md
 â”£ ğŸ““ Transformer_Attention.ipynb
 â”£ ğŸ“„ Transformers-attention.docx
 â”— ğŸ“ images/ (optional example screenshots)

ğŸš€ Future Enhancements

Add RoBERTa, DistilBERT, ALBERT comparisons

Combine attention across layers

Visualize attention before vs after fine-tuning

Export visualizations automatically

ğŸ“œ License

MIT License â€” free to use, modify, and share.
